{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlogprobs import extract_logprobs, ToyModel\n",
    "toy_model = ToyModel(50, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_logprobs = extract_logprobs(toy_model, \"i like pie\", method=\"bisection\",bias=100, eps = 1e-5)\n",
    "print (\"logits:\", extracted_logprobs[0], \"steps:\", extracted_logprobs[1])\n",
    "import numpy as np\n",
    "\n",
    "diff = toy_model.logits - extracted_logprobs[0]\n",
    "diff -= diff[0]\n",
    "print (\"error:\",sum(np.abs(diff))/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lige_exact_logprobs(\n",
    "    model,\n",
    "    prefix: str,\n",
    "    eps: float = 1e-8,\n",
    "    bias: float = 40.0,\n",
    "):\n",
    "    \"\"\"啊啊啊李哥你自己想想啊，我把框架给你搭好\n",
    "    input:\n",
    "       model: class Model: (你只用得到argmax)\n",
    "            This class wraps the model API. It can take text and a logit bias and return text outputs.\n",
    "            Methods:\n",
    "            def vocab_size() -> int:\n",
    "            def argmax(self, prefix: str, logit_bias: Dict[str, float] = {}) -> int:\n",
    "            def topk(self, prefix: str, logit_bias: Dict[str, float] = {}) -> Dict[int, float]:\n",
    "            def median_topk(self, k, *args, **kwargs)\n",
    "            def median_argmax(self, k, *args, **kwargs)\n",
    "        \n",
    "        prefix: str 你不用管，直接扔到model里就行\n",
    "        eps: float\n",
    "            epsilon, the tolerence for the error of the final result from the true logits\n",
    "        bias: the lower bound of error, i.e. the numbers are in range (0,-bias)\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = model.vocab_size\n",
    "    step = 0\n",
    "\n",
    "    # Your code: intialization\n",
    "    logits = np.zeros(vocab_size)\n",
    "\n",
    "    while True: # Your code: change the stopping condition here\n",
    "\n",
    "        # Your code: pre-processing before query, prepare logit_bias\n",
    "        logit_bias = {}\n",
    "\n",
    "        # query\n",
    "        sampled = model.argmax(prefix, logit_bias)\n",
    "\n",
    "        # Your code: post-processing\n",
    "\n",
    "        \n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    return logits, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lige_logprobs = lige_exact_logprobs(toy_model, \"\")\n",
    "print (\"logits:\", lige_logprobs[0], \"steps:\", lige_logprobs[1])\n",
    "lige_diff = toy_model.logits - lige_logprobs[0]\n",
    "lige_diff -= diff[0]\n",
    "print (\"error:\",sum(np.abs(lige_diff))/len(lige_diff))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
